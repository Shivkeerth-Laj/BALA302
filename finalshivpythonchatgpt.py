# -*- coding: utf-8 -*-
"""shivpythonchatgpt

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10VcQYn72--VTuBRIfQfmFLW8_Gt1Qafb
"""

import pandas as pd

# Load the dataset with your specific file name
df = pd.read_csv('/content/drive/MyDrive/PYTHON PROJECT LAPD4/Crime_Data_from_2020_to_Present.csv')

# Columns to remove
columns_to_remove = [
    'DR_NO',
    'Date Rptd',
    'AREA',
    'Rpt Dist No',
    'Part 1-2',
    'Crm Cd',
    'Mocodes',
    'Vict Descent',
    'Premis Cd',
    'Weapon Used Cd',
    'Status',
    'Crm Cd 1',
    'Crm Cd 2',
    'Crm Cd 3',
    'Crm Cd 4',
    'Cross Street'
]

# Remove the specified columns
df_cleaned = df.drop(columns=columns_to_remove, errors='ignore')

# Save the cleaned dataframe to a new file
df_cleaned.to_csv('/content/drive/MyDrive/PYTHON PROJECT LAPD4/cleaned_crime_data.csv', index=False)

print("Columns removed successfully. Remaining columns:")
print(df_cleaned.columns.tolist())

# Function to convert date to DD/MM/YYYY format
def standardize_date(date_str):
    try:
        # Parse the date (assuming it's in a standard format like MM/DD/YYYY or YYYY-MM-DD)
        date_obj = pd.to_datetime(date_str)
        # Format to DD/MM/YYYY
        return date_obj.strftime('%d/%m/%Y')
    except:
        return date_str  # Return original if conversion fails

# Apply the function to standardize the DATE OCC column
df_cleaned['DATE OCC'] = df_cleaned['DATE OCC'].apply(standardize_date)

# Save the cleaned and standardized dataframe to a new file
df_cleaned.to_csv('/content/drive/MyDrive/PYTHON PROJECT LAPD4/standardized_crime_data.csv', index=False)

print("Columns removed and date format standardized successfully.")
print("Sample of converted dates:")
print(df_cleaned['DATE OCC'].head(10))

# Save the cleaned and standardized dataframe to the new file name
df_cleaned.to_csv('/content/drive/MyDrive/PYTHON PROJECT LAPD4/Cleaned crime data 2.csv', index=False)

print("Data processed successfully and saved as 'Cleaned crime data 2.csv'")

# Add code to download the file
from google.colab import files
files.download('/content/drive/MyDrive/PYTHON PROJECT LAPD4/Cleaned crime data 2.csv')

import pandas as pd
from google.colab import files

# Load the dataset
file_path = "/content/drive/MyDrive/PYTHON PROJECT LAPD4/Cleaned crime data 2.csv"  # Ensure your dataset is in Colab
df = pd.read_csv(file_path)

# Clean the 'Vict Sex' column
df['Vict Sex'] = df['Vict Sex'].replace(['-', 'H'], 'X').fillna('X')

print("Vict Sex column cleaned successfully! The updated file is downloaded.")

# Save the cleaned dataset
cleaned_file_path = "/content/drive/MyDrive/PYTHON PROJECT LAPD4/Cleaned_crime_data_final.csv"
df.to_csv(cleaned_file_path, index=False)

# Download the cleaned file
files.download(cleaned_file_path)

# Remove rows where 'Vict Age' is -4, -3, -2, or -1
df = df[~df['Vict Age'].isin([-4, -3, -2, -1])]
# Save the updated dataset
updated_file_path = "/content/drive/MyDrive/PYTHON PROJECT LAPD4/Cleaned_Crime_Data_Updated.csv"
df.to_csv(updated_file_path, index=False)
# Display a sample of the cleaned data


# Reload the dataset
file_path = "/content/drive/MyDrive/PYTHON PROJECT LAPD4/Cleaned_Crime_Data_Updated.csv"
df = pd.read_csv(file_path)

# Function to format TIME OCC as XX:XX
def format_time(time_value):
    time_str = str(time_value).zfill(4)  # Ensure it's a 4-digit string
    return f"{time_str[:2]}:{time_str[2:]}"

# Apply the function to the TIME OCC column
df['TIME OCC'] = df['TIME OCC'].apply(format_time)

# Save the updated dataset
final_file_path = "/content/drive/MyDrive/PYTHON PROJECT LAPD4/Cleaned_Crime_Data_Final_Format.csv"
df.to_csv(final_file_path, index=False)

# Display a sample of the cleaned data


# Remove rows where 'Weapon Desc' is blank or NaN
df = df.dropna(subset=['Weapon Desc'])

# Save the updated dataset
filtered_file_path = "/content/drive/MyDrive/PYTHON PROJECT LAPD4/Cleaned_Crime_Data_Weapons_Filtered.csv"
df.to_csv(filtered_file_path, index=False)

# Display a sample of the cleaned data


# Reload necessary libraries
import pandas as pd

# Load the dataset
file_path = "/content/drive/MyDrive/PYTHON PROJECT LAPD4/Cleaned_Crime_Data_Final_Format.csv"
df = pd.read_csv(file_path)

# Remove rows where 'Weapon Desc' is blank or NaN
df = df.dropna(subset=['Weapon Desc'])

# Save the updated dataset
filtered_file_path = "/content/drive/MyDrive/PYTHON PROJECT LAPD4/Cleaned_Crime_Data_Weapons_Filtered.csv"
df.to_csv(filtered_file_path, index=False)

# Display a sample of the cleaned data


import pandas as pd

# Load the dataset
file_path = "/content/drive/MyDrive/PYTHON PROJECT LAPD4/Cleaned_Crime_Data_Weapons_Filtered.csv"
df = pd.read_csv(file_path)

# Remove rows where 'Premis Desc' is blank or NaN
df_cleaned = df[df['Premis Desc'].notna() & (df['Premis Desc'] != '')]

# Save the cleaned dataset
cleaned_file_path = "/content/drive/MyDrive/PYTHON PROJECT LAPD4/Cleaned_Crime_Data_Weapons_Filtered_Cleaned.csv"
df_cleaned.to_csv(cleaned_file_path, index=False)

# Provide download link
cleaned_file_path

# Remove rows where 'LAT' and 'LON' are 0
df_cleaned = df_cleaned[(df_cleaned['LAT'] != 0) & (df_cleaned['LON'] != 0)]

# Save the further cleaned dataset
further_cleaned_file_path = "/content/drive/MyDrive/PYTHON PROJECT LAPD4/Cleaned_Crime_Data_Weapons_Filtered_Further_Cleaned.csv"
df_cleaned.to_csv(further_cleaned_file_path, index=False)

# Provide download link
files.download(further_cleaned_file_path)



# Provide download link
files.download(further_cleaned_file_path)

# Install necessary libraries (if not installed)
!pip install pandas plotly

# Import necessary libraries
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go

# Mount Google Drive (if not already mounted)
from google.colab import drive
drive.mount('/content/drive')

# Load the dataset using your specific file path
file_path = "/content/drive/MyDrive/PYTHON PROJECT LAPD4/Cleaned_Crime_Data_Weapons_Filtered_Further_Cleaned.csv"
crime_data = pd.read_csv(file_path)

# Convert 'DATE OCC' to datetime format
crime_data['DATE OCC'] = pd.to_datetime(crime_data['DATE OCC'], errors='coerce')

# Aggregate crime counts per month
crime_data['Year-Month'] = crime_data['DATE OCC'].dt.to_period('M')  # Convert to Year-Month format
crime_trend = crime_data.groupby('Year-Month').size().reset_index(name='Crime Count')

# Convert 'Year-Month' to string for plotting
crime_trend['Year-Month'] = crime_trend['Year-Month'].astype(str)

# Create interactive plot with Plotly
fig = px.line(crime_trend, x='Year-Month', y='Crime Count',
              title='Crime Rate Trends Before, During, and After COVID-19',
              labels={'Year-Month': 'Year-Month', 'Crime Count': 'Total Crimes'},
              markers=True)

# Define COVID timeline markers
covid_start = '2020-03'
covid_end = '2021-12'

# Create a layout with shapes for vertical lines
fig.update_layout(
    shapes=[
        # COVID start line
        dict(
            type="line",
            xref="x",
            yref="paper",
            x0=covid_start,
            y0=0,
            x1=covid_start,
            y1=1,
            line=dict(
                color="red",
                width=2,
                dash="dash",
            )
        ),
        # COVID end line
        dict(
            type="line",
            xref="x",
            yref="paper",
            x0=covid_end,
            y0=0,
            x1=covid_end,
            y1=1,
            line=dict(
                color="green",
                width=2,
                dash="dash",
            )
        )
    ]
)

# Add annotations for COVID periods
fig.add_annotation(
    x=covid_start,
    y=1,
    text="COVID Start (Mar 2020)",
    showarrow=True,
    arrowhead=1,
    ax=0,
    ay=-40
)

fig.add_annotation(
    x=covid_end,
    y=1,
    text="COVID End (Dec 2021)",
    showarrow=True,
    arrowhead=1,
    ax=0,
    ay=-40
)

# Customize hover data
fig.update_traces(
    hovertemplate='<b>Date</b>: %{x}<br><b>Crime Count</b>: %{y}<extra></extra>'
)

# Improve layout
fig.update_layout(
    xaxis_title='Year-Month',
    yaxis_title='Total Crimes',
    hovermode='closest',
    xaxis=dict(tickangle=45)
)

# Display the plot
fig.show()

from google.colab import drive
drive.mount('/content/drive')

# Install necessary libraries (if not installed)
!pip install pandas seaborn matplotlib plotly

# Import necessary libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px

# Mount Google Drive (if using Google Drive for dataset storage)
from google.colab import drive
drive.mount('/content/drive')

# Load the dataset using your specific file path
file_path = "/content/drive/MyDrive/PYTHON PROJECT LAPD4/Cleaned_Crime_Data_Weapons_Filtered_Further_Cleaned.csv"
crime_data = pd.read_csv(file_path)

# Convert 'DATE OCC' to datetime format
crime_data['DATE OCC'] = pd.to_datetime(crime_data['DATE OCC'], errors='coerce')

# Filter out rows with NaT in DATE OCC
crime_data = crime_data.dropna(subset=['DATE OCC'])

# Define Pandemic Phases
def categorize_pandemic_phase(date):
    if date < pd.Timestamp("2020-03-01"):
        return "Pre-COVID"
    elif pd.Timestamp("2020-03-01") <= date <= pd.Timestamp("2021-12-31"):
        return "During-COVID"
    else:
        return "Post-COVID"

crime_data["Pandemic Phase"] = crime_data["DATE OCC"].apply(categorize_pandemic_phase)

# To prevent the heatmap from being too crowded, get top 10 crime types
top_crimes = crime_data["Crm Cd Desc"].value_counts().nlargest(10).index.tolist()
crime_data_filtered = crime_data[crime_data["Crm Cd Desc"].isin(top_crimes)]

# Aggregate crime count by Crime Type & Pandemic Phase
crime_type_trends = crime_data_filtered.groupby(["Pandemic Phase", "Crm Cd Desc"]).size().reset_index(name="Crime Count")

# Ensure Pandemic Phase is ordered correctly
phase_order = ["Pre-COVID", "During-COVID", "Post-COVID"]
crime_type_trends["Pandemic Phase"] = pd.Categorical(
    crime_type_trends["Pandemic Phase"],
    categories=phase_order,
    ordered=True
)
crime_type_trends = crime_type_trends.sort_values("Pandemic Phase")

# Stacked Bar Chart: Crime Type Distribution Before, During, and After COVID
fig = px.bar(
    crime_type_trends,
    x="Pandemic Phase",
    y="Crime Count",
    color="Crm Cd Desc",
    title="Crime Type Distribution Before, During, and After COVID",
    labels={"Pandemic Phase": "Pandemic Phase", "Crime Count": "Total Crimes", "Crm Cd Desc": "Crime Type"},
    barmode="stack",
    height=600
)

# Improve layout
fig.update_layout(
    legend_title="Crime Type",
    xaxis_title="Pandemic Phase",
    yaxis_title="Total Crimes",
    legend=dict(orientation="h", yanchor="bottom", y=-0.3, xanchor="center", x=0.5)
)

# Show interactive stacked bar chart
fig.show()

# Install necessary libraries (if not installed)
!pip install pandas folium geopandas plotly

# Import necessary libraries
import pandas as pd
import folium
from folium.plugins import HeatMap, MarkerCluster
import plotly.express as px
from google.colab import drive

# Mount Google Drive (if using Google Drive for dataset storage)
drive.mount('/content/drive')

# Load the dataset
file_path = "/content/drive/MyDrive/PYTHON PROJECT LAPD4/Cleaned_Crime_Data_Weapons_Filtered_Further_Cleaned.csv"
crime_data = pd.read_csv(file_path)

# Convert 'DATE OCC' to datetime format
crime_data['DATE OCC'] = pd.to_datetime(crime_data['DATE OCC'], errors='coerce')

# Define Pandemic Phases
def categorize_pandemic_phase(date):
    if pd.isna(date):
        return "Unknown"
    elif date < pd.Timestamp("2020-03-01"):
        return "Pre-COVID"
    elif pd.Timestamp("2020-03-01") <= date <= pd.Timestamp("2021-12-31"):
        return "During-COVID"
    else:
        return "Post-COVID"

crime_data["Pandemic Phase"] = crime_data["DATE OCC"].apply(categorize_pandemic_phase)

# Extract relevant columns
crime_map_data = crime_data[['LAT', 'LON', 'AREA NAME', 'Pandemic Phase']].copy()
crime_map_data.dropna(subset=['LAT', 'LON'], inplace=True)  # Remove rows with missing lat/lon


# Alternative to Choropleth: Create a scatter map of crimes by area

# Aggregate crime count per area
crime_area_counts = crime_map_data.groupby("AREA NAME").size().reset_index(name="Crime Count")

# Create a scatter mapbox (requires no GeoJSON)
fig = px.scatter_mapbox(
    crime_map_data,
    lat="LAT",
    lon="LON",
    color="Pandemic Phase",
    size_max=15,
    zoom=9,
    mapbox_style="carto-positron",
    title="Crime Distribution Across Los Angeles",
    color_discrete_map={
        "Pre-COVID": "blue",
        "During-COVID": "red",
        "Post-COVID": "green"
    },
    opacity=0.5
)

# Update layout
fig.update_layout(margin={"r":0,"t":40,"l":0,"b":0}, height=600)

# Show interactive map
fig.show()


# Cluster Map: Crime Hotspots Before, During, and After COVID

# Define a base map centered on Los Angeles
la_map = folium.Map(location=[34.0522, -118.2437], zoom_start=10)

# Create separate datasets for each pandemic phase
phases = ['Pre-COVID', 'During-COVID', 'Post-COVID']
colors = {'Pre-COVID': 'blue', 'During-COVID': 'red', 'Post-COVID': 'green'}

# Sample the data to avoid browser performance issues (take 5% of rows)
sample_ratio = 0.05
crime_map_data_sampled = crime_map_data.sample(frac=sample_ratio, random_state=42)

for phase in phases:
    phase_data = crime_map_data_sampled[crime_map_data_sampled["Pandemic Phase"] == phase]

    # Create marker cluster
    marker_cluster = MarkerCluster(name=f"Crime Hotspots ({phase})").add_to(la_map)

    # Add crime locations to the map
    for idx, row in phase_data.iterrows():
        folium.CircleMarker(
            location=[row["LAT"], row["LON"]],
            radius=3,
            color=colors[phase],
            fill=True,
            fill_color=colors[phase],
            fill_opacity=0.5,
            popup=f"Area: {row['AREA NAME']}"
        ).add_to(marker_cluster)

# Add layer control to toggle pandemic phases
folium.LayerControl().add_to(la_map)

# Display the map
la_map

# Install necessary libraries (if not installed)
!pip install pandas numpy seaborn matplotlib scikit-learn xgboost tensorflow

# Import required libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from google.colab import drive

# Mount Google Drive (if not already mounted)
drive.mount('/content/drive')

# Load the dataset
file_path = "/content/drive/MyDrive/PYTHON PROJECT LAPD4/Cleaned_Crime_Data_Weapons_Filtered_Further_Cleaned.csv"
crime_data = pd.read_csv(file_path)

# Convert 'DATE OCC' to datetime format
crime_data['DATE OCC'] = pd.to_datetime(crime_data['DATE OCC'], errors='coerce')

# Drop rows with missing dates
crime_data = crime_data.dropna(subset=['DATE OCC'])

# Feature Engineering: Categorize Pandemic Phases
def categorize_pandemic_phase(date):
    if date < pd.Timestamp("2020-03-01"):
        return "Pre-COVID"
    elif pd.Timestamp("2020-03-01") <= date <= pd.Timestamp("2021-12-31"):
        return "During-COVID"
    else:
        return "Post-COVID"

crime_data["Pandemic Phase"] = crime_data["DATE OCC"].apply(categorize_pandemic_phase)

# Aggregate crime count per area and time period
crime_data['Year-Month'] = crime_data['DATE OCC'].dt.to_period('M')
crime_counts = crime_data.groupby(["Year-Month", "AREA NAME"]).size().reset_index(name="Crime Count")

# Convert Period to string for easier handling
crime_counts['Year-Month'] = crime_counts['Year-Month'].astype(str)

# Create time-based features for better prediction
crime_counts['Year'] = crime_counts['Year-Month'].str.split('-').str[0].astype(int)
crime_counts['Month'] = crime_counts['Year-Month'].str.split('-').str[1].astype(int)

# One-Hot Encode Categorical Variables
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
encoded_features = pd.DataFrame(encoder.fit_transform(crime_counts[['AREA NAME']]))
encoded_features.columns = encoder.get_feature_names_out(['AREA NAME'])

# Join the encoded features with the original dataframe
crime_counts_encoded = crime_counts.drop(columns=['AREA NAME']).join(encoded_features)

# Prepare features for model training (excluding Year-Month which is now a string)
X = crime_counts_encoded.drop(columns=['Year-Month', 'Crime Count'])
y = crime_counts_encoded['Crime Count']

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Linear Regression

print("Training Linear Regression model...")
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)
y_pred_lin = lin_reg.predict(X_test)
mse_lin = mean_squared_error(y_test, y_pred_lin)
r2_lin = r2_score(y_test, y_pred_lin)
print(f"Linear Regression - MSE: {mse_lin:.2f}, R¬≤: {r2_lin:.4f}")


# Random Forest Regressor

print("\nTraining Random Forest model...")
rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)
rf_reg.fit(X_train, y_train)
y_pred_rf = rf_reg.predict(X_test)
mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)
print(f"Random Forest - MSE: {mse_rf:.2f}, R¬≤: {r2_rf:.4f}")


# XGBoost Regressor

print("\nTraining XGBoost model...")
xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)
xgb_reg.fit(X_train, y_train)
y_pred_xgb = xgb_reg.predict(X_test)
mse_xgb = mean_squared_error(y_test, y_pred_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)
print(f"XGBoost - MSE: {mse_xgb:.2f}, R¬≤: {r2_xgb:.4f}")


# LSTM (Deep Learning)

print("\nPreparing data for LSTM model...")
try:
    # Reshape data for LSTM (samples, time steps, features)
    X_train_lstm = np.array(X_train).reshape((X_train.shape[0], 1, X_train.shape[1]))
    X_test_lstm = np.array(X_test).reshape((X_test.shape[0], 1, X_test.shape[1]))

    print("Training LSTM model...")
    lstm_model = Sequential([
        LSTM(50, activation='relu', input_shape=(1, X_train.shape[1])),
        Dense(25, activation='relu'),
        Dense(1)
    ])

    lstm_model.compile(optimizer='adam', loss='mse')
    lstm_model.fit(X_train_lstm, y_train, epochs=10, batch_size=32, verbose=1)

    y_pred_lstm = lstm_model.predict(X_test_lstm).flatten()
    mse_lstm = mean_squared_error(y_test, y_pred_lstm)
    r2_lstm = r2_score(y_test, y_pred_lstm)
    print(f"LSTM - MSE: {mse_lstm:.2f}, R¬≤: {r2_lstm:.4f}")
except Exception as e:
    print(f"Error training LSTM model: {e}")
    # Set fallback values if LSTM fails
    mse_lstm = float('inf')
    r2_lstm = -float('inf')


#  Model Evaluation & Selection

results = pd.DataFrame({
    "Model": ["Linear Regression", "Random Forest", "XGBoost", "LSTM"],
    "MSE": [mse_lin, mse_rf, mse_xgb, mse_lstm],
    "R¬≤ Score": [r2_lin, r2_rf, r2_xgb, r2_lstm]
})

print("\n Model Performance Results:")
print(results)

# Select the best model based on lowest MSE and highest R¬≤ Score
best_model_idx = results['R¬≤ Score'].idxmax()
best_model = results.loc[best_model_idx, "Model"]
print(f"\n‚úÖ Best Model Selected: {best_model}")


# Visualize Model Performance

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.bar(results['Model'], results['MSE'], color=['blue', 'green', 'orange', 'red'])
plt.title('Mean Squared Error (Lower is Better)')
plt.xticks(rotation=45)

plt.subplot(1, 2, 2)
plt.bar(results['Model'], results['R¬≤ Score'], color=['blue', 'green', 'orange', 'red'])
plt.title('R¬≤ Score (Higher is Better)')
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()


# Save the Best Model

import pickle
model_dict = {
    "Linear Regression": lin_reg,
    "Random Forest": rf_reg,
    "XGBoost": xgb_reg
}

if best_model == "LSTM" and r2_lstm > -float('inf'):
    # Save LSTM model separately as it requires a different format
    lstm_model.save('best_crime_model_lstm.h5')
    print("\n‚úÖ LSTM Model saved as 'best_crime_model_lstm.h5'")
else:
    # Save the best sklearn-based model
    pickle.dump(model_dict[best_model], open("/content/drive/MyDrive/PYTHON PROJECT LAPD4/best_crime_model.sav", "wb"))
    print(f"\n‚úÖ {best_model} model saved as 'best_crime_model.sav'")

# Generate predictive insights
print("\n Feature Importance Analysis:")
if best_model == "Random Forest":
    feature_importance = pd.DataFrame({
        'Feature': X.columns,
        'Importance': rf_reg.feature_importances_
    }).sort_values('Importance', ascending=False)
    print(feature_importance.head(10))

    plt.figure(figsize=(10, 6))
    sns.barplot(x='Importance', y='Feature', data=feature_importance.head(10))
    plt.title('Top 10 Most Important Features')
    plt.tight_layout()
    plt.show()
elif best_model == "XGBoost":
    feature_importance = pd.DataFrame({
        'Feature': X.columns,
        'Importance': xgb_reg.feature_importances_
    }).sort_values('Importance', ascending=False)
    print(feature_importance.head(10))

    plt.figure(figsize=(10, 6))
    sns.barplot(x='Importance', y='Feature', data=feature_importance.head(10))
    plt.title('Top 10 Most Important Features')
    plt.tight_layout()
    plt.show()

# Install necessary libraries (if not installed)
!pip install pandas numpy scikit-learn plotly

# Import required libraries
import pandas as pd
import numpy as np
import pickle
from google.colab import drive
from sklearn.preprocessing import OneHotEncoder
import plotly.express as px

# Mount Google Drive (if not already mounted)
drive.mount('/content/drive')

# Load the trained Random Forest model
model_path = "/content/drive/MyDrive/PYTHON PROJECT LAPD4/best_crime_model.sav"
loaded_model = pickle.load(open(model_path, "rb"))
print(" Random Forest model loaded successfully")

#  Load the dataset again to extract feature names
file_path = "/content/drive/MyDrive/PYTHON PROJECT LAPD4/Cleaned_Crime_Data_Weapons_Filtered_Further_Cleaned.csv"
crime_data = pd.read_csv(file_path)

# Prepare input features (same feature engineering as training)
crime_data['DATE OCC'] = pd.to_datetime(crime_data['DATE OCC'], errors='coerce')
crime_data = crime_data.dropna(subset=['DATE OCC'])  # Remove rows with missing dates

# Create time-based features
crime_data['Year-Month'] = crime_data['DATE OCC'].dt.to_period('M')
crime_data['Year'] = crime_data['DATE OCC'].dt.year
crime_data['Month'] = crime_data['DATE OCC'].dt.month

# Aggregate crime count per area and time period
crime_counts = crime_data.groupby(["Year-Month", "AREA NAME"]).size().reset_index(name="Crime Count")

# Convert Period to string for easier handling
crime_counts['Year-Month'] = crime_counts['Year-Month'].astype(str)

# Create time-based features for prediction
crime_counts['Year'] = crime_counts['Year-Month'].str.split('-').str[0].astype(int)
crime_counts['Month'] = crime_counts['Year-Month'].str.split('-').str[1].astype(int)

# One-Hot Encode Categorical Variables
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
encoded_features = pd.DataFrame(encoder.fit_transform(crime_counts[['AREA NAME']]))
encoded_features.columns = encoder.get_feature_names_out(['AREA NAME'])

# Join the encoded features with numeric features
crime_counts_encoded = crime_counts.drop(columns=['AREA NAME', 'Year-Month', 'Crime Count']).join(encoded_features)

# Get column names used by the model
X_columns = crime_counts_encoded.columns

# Display available areas for user selection
unique_areas = sorted(crime_data["AREA NAME"].unique().tolist())
print("\n Available Areas:")
for i, area in enumerate(unique_areas, 1):
    print(f"{i}. {area}")


# Predict Crime Count for New Data (Manual Input)

try:
    selected_area_idx = int(input("\nEnter the area number (1-21): ")) - 1
    selected_area = unique_areas[selected_area_idx]

    selected_year = int(input("Enter year (e.g., 2023): "))
    selected_month = int(input("Enter month (1-12): "))

    # Create a new input row (default 0 for all features)
    new_input = pd.DataFrame(np.zeros((1, len(X_columns))), columns=X_columns)

    # Set year and month
    if 'Year' in new_input.columns:
        new_input['Year'] = selected_year
    if 'Month' in new_input.columns:
        new_input['Month'] = selected_month

    # Set the selected area feature to 1
    area_col = f"AREA NAME_{selected_area}"
    if area_col in new_input.columns:
        new_input[area_col] = 1
    else:
        print(f"Warning: Area '{selected_area}' not found in training data columns.")
        # Handle unknown area
        # We continue anyway since OneHotEncoder was set with handle_unknown='ignore'

    # Make Prediction
    predicted_crime_count = loaded_model.predict(new_input)[0]
    print(f"\n‚úÖ Predicted Crime Count for {selected_area} in {selected_month}/{selected_year}: {round(predicted_crime_count)}")

except Exception as e:
    print(f"Error making prediction: {e}")
    print("Using default values instead...")

    # Use default values for demonstration
    selected_area = unique_areas[0]
    selected_year = 2023
    selected_month = 1

    # Create input with default values
    new_input = pd.DataFrame(np.zeros((1, len(X_columns))), columns=X_columns)
    if 'Year' in new_input.columns:
        new_input['Year'] = selected_year
    if 'Month' in new_input.columns:
        new_input['Month'] = selected_month

    area_col = f"AREA NAME_{selected_area}"
    if area_col in new_input.columns:
        new_input[area_col] = 1

    predicted_crime_count = loaded_model.predict(new_input)[0]
    print(f"\n‚úÖ Predicted Crime Count for {selected_area} in {selected_month}/{selected_year}: {round(predicted_crime_count)}")


# Predict Crime Counts for All Areas in Future Period

print("\n Predicting Crime Counts for All Areas in Future Period:")

future_year = selected_year  # Use the same year as before or modify
future_month = selected_month  # Use the same month as before or modify

print(f"\n Generating predictions for {future_month}/{future_year} across all areas...")

# Create predictions for all areas for the specified future period
future_predictions = []

for area in unique_areas:
    future_input = pd.DataFrame(np.zeros((1, len(X_columns))), columns=X_columns)

    # Set time features
    if 'Year' in future_input.columns:
        future_input['Year'] = future_year
    if 'Month' in future_input.columns:
        future_input['Month'] = future_month

    # Set area feature
    area_col = f"AREA NAME_{area}"
    if area_col in future_input.columns:
        future_input[area_col] = 1

    # Make prediction
    pred_count = round(loaded_model.predict(future_input)[0])
    future_predictions.append({'Area': area, 'Predicted Crime Count': pred_count})

# Create dataframe and display results
prediction_df = pd.DataFrame(future_predictions)
prediction_df = prediction_df.sort_values('Predicted Crime Count', ascending=False)

print("\n Predicted Crime Counts by Area (Highest to Lowest):")
print(prediction_df)


# Visualize Predictions with Interactive Features

try:
    # Create interactive bar chart with Plotly
    fig = px.bar(
        prediction_df,
        x='Predicted Crime Count',
        y='Area',
        orientation='h',
        color='Predicted Crime Count',
        color_continuous_scale='Viridis',
        title=f'Predicted Crime Counts by Area for {future_month}/{future_year}'
    )

    # Enhance hover information to show date and crime count
    fig.update_traces(
        hovertemplate='<b>%{y}</b><br>Date: %{customdata}<br>Predicted Crimes: %{x}<extra></extra>',
        customdata=[f"{future_month}/{future_year}" for _ in range(len(prediction_df))]
    )

    # Improve layout
    fig.update_layout(
        height=600,
        width=900,
        xaxis_title='Predicted Number of Crimes',
        yaxis_title='Area',
        coloraxis_showscale=True,
        hovermode='closest'
    )

    # Display the interactive plot
    fig.show()

    print("\n‚úÖ Interactive Analysis Complete!")

except Exception as e:
    print(f"Error creating interactive visualization: {e}")
    print("Make sure Plotly is installed with: pip install plotly")

    # Fallback to non-interactive visualization using matplotlib/seaborn
    try:
        import matplotlib.pyplot as plt
        import seaborn as sns

        print("Falling back to static visualization...")
        plt.figure(figsize=(12, 8))
        sns.barplot(x='Predicted Crime Count', y='Area', data=prediction_df, palette='viridis')
        plt.title(f'Predicted Crime Counts by Area for {future_month}/{future_year}')
        plt.xlabel('Predicted Number of Crimes')
        plt.ylabel('Area')
        plt.tight_layout()
        plt.show()

    except Exception as e2:
        print(f"Error creating static visualization: {e2}")
        print("Skipping visualization, but prediction data is available above.")

# Install necessary libraries
!pip install pandas numpy seaborn matplotlib scikit-learn xgboost

# Import required libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.cluster import KMeans
import pickle
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Load the dataset
file_path = "/content/drive/MyDrive/PYTHON PROJECT LAPD4/Cleaned_Crime_Data_Weapons_Filtered_Further_Cleaned.csv"
crime_data = pd.read_csv(file_path)

# Data check and cleaning
print(f"Original data shape: {crime_data.shape}")
crime_data = crime_data.dropna(subset=['DATE OCC'])  # Drop rows with missing dates
print(f"Data shape after dropping null dates: {crime_data.shape}")

# Convert 'DATE OCC' to datetime format
crime_data['DATE OCC'] = pd.to_datetime(crime_data['DATE OCC'], errors='coerce')
# Drop any rows where date conversion failed
crime_data = crime_data.dropna(subset=['DATE OCC'])
print(f"Data shape after converting dates: {crime_data.shape}")

# Feature Engineering: Categorize Pandemic Phases
def categorize_pandemic_phase(date):
    if date < pd.Timestamp("2020-03-01"):
        return "Pre-COVID"
    elif pd.Timestamp("2020-03-01") <= date <= pd.Timestamp("2021-12-31"):
        return "During-COVID"
    else:
        return "Post-COVID"

crime_data["Pandemic Phase"] = crime_data["DATE OCC"].apply(categorize_pandemic_phase)

# Create time features
crime_data['Year'] = crime_data['DATE OCC'].dt.year
crime_data['Month'] = crime_data['DATE OCC'].dt.month
crime_data['Year-Month'] = crime_data['DATE OCC'].dt.to_period('M')

# Aggregate crime count per area and time period
crime_counts = crime_data.groupby(["Year-Month", "AREA NAME"]).size().reset_index(name="Crime Count")

# Convert Period to string for easier handling
crime_counts['Year-Month'] = crime_counts['Year-Month'].astype(str)

# Extract year and month from Year-Month string
crime_counts['Year'] = crime_counts['Year-Month'].str.split('-').str[0].astype(int)
crime_counts['Month'] = crime_counts['Year-Month'].str.split('-').str[1].astype(int)

# Define High-Risk Areas (Top 10% crime areas labeled as 1, others as 0)
crime_counts["High-Risk"] = (crime_counts["Crime Count"] >= crime_counts["Crime Count"].quantile(0.90)).astype(int)

# Check class balance
print("\nClass distribution for High-Risk areas:")
print(crime_counts["High-Risk"].value_counts(normalize=True) * 100)

# One-Hot Encode Categorical Variables
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
encoded_features = pd.DataFrame(encoder.fit_transform(crime_counts[['AREA NAME']]))
encoded_features.columns = encoder.get_feature_names_out(['AREA NAME'])

# Join the encoded features with numeric features
crime_counts_encoded = crime_counts.drop(columns=['AREA NAME', 'Year-Month']).join(encoded_features)

# Define input features (X) and target (y)
X = crime_counts_encoded.drop(columns=['Crime Count', 'High-Risk'])
y = crime_counts_encoded['High-Risk']

print(f"\nFeatures used for modeling: {X.columns.tolist()}")
print(f"Features shape: {X.shape}, Target shape: {y.shape}")

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Check proportions in train and test sets
print("\nTrain set class distribution:")
print(pd.Series(y_train).value_counts(normalize=True) * 100)
print("\nTest set class distribution:")
print(pd.Series(y_test).value_counts(normalize=True) * 100)


# 1 Logistic Regression

print("\n Training Logistic Regression...")
log_reg = LogisticRegression(solver='liblinear', class_weight='balanced', max_iter=1000)
log_reg.fit(X_train, y_train)
y_pred_log = log_reg.predict(X_test)

# Evaluate Logistic Regression
accuracy_log = accuracy_score(y_test, y_pred_log)
precision_log = precision_score(y_test, y_pred_log)
recall_log = recall_score(y_test, y_pred_log)
f1_log = f1_score(y_test, y_pred_log)


# 2 Decision Tree Classifier

print(" Training Decision Tree Classifier...")
dt_clf = DecisionTreeClassifier(max_depth=5, random_state=42, class_weight='balanced')
dt_clf.fit(X_train, y_train)
y_pred_dt = dt_clf.predict(X_test)

# Evaluate Decision Tree
accuracy_dt = accuracy_score(y_test, y_pred_dt)
precision_dt = precision_score(y_test, y_pred_dt)
recall_dt = recall_score(y_test, y_pred_dt)
f1_dt = f1_score(y_test, y_pred_dt)


# 3 Random Forest Classifier

print(" Training Random Forest Classifier...")
rf_clf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
rf_clf.fit(X_train, y_train)
y_pred_rf = rf_clf.predict(X_test)

# Evaluate Random Forest
accuracy_rf = accuracy_score(y_test, y_pred_rf)
precision_rf = precision_score(y_test, y_pred_rf)
recall_rf = recall_score(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf)


# 4 K-Means Clustering (Unsupervised Learning)

print(" Training K-Means Clustering...")
# Standardize features for K-means (important!)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
kmeans.fit(X_train_scaled)

# Assign cluster labels as predictions
y_pred_kmeans_raw = kmeans.predict(X_test_scaled)

# Map clusters to actual labels more intelligently
# We want cluster with higher average crime count to be labeled as high-risk (1)
cluster_df = pd.DataFrame({'cluster': kmeans.labels_, 'high_risk': y_train.values})
cluster_risk_mapping = cluster_df.groupby('cluster')['high_risk'].mean()
high_risk_cluster = cluster_risk_mapping.idxmax()

# Apply mapping to test predictions
y_pred_kmeans = (y_pred_kmeans_raw == high_risk_cluster).astype(int)

# Evaluate K-Means Clustering
accuracy_kmeans = accuracy_score(y_test, y_pred_kmeans)
precision_kmeans = precision_score(y_test, y_pred_kmeans, zero_division=0)
recall_kmeans = recall_score(y_test, y_pred_kmeans, zero_division=0)
f1_kmeans = f1_score(y_test, y_pred_kmeans, zero_division=0)


# Model Evaluation & Selection

results = pd.DataFrame({
    "Model": ["Logistic Regression", "Decision Tree", "Random Forest", "K-Means Clustering"],
    "Accuracy": [accuracy_log, accuracy_dt, accuracy_rf, accuracy_kmeans],
    "Precision": [precision_log, precision_dt, precision_rf, precision_kmeans],
    "Recall": [recall_log, recall_dt, recall_rf, recall_kmeans],
    "F1 Score": [f1_log, f1_dt, f1_rf, f1_kmeans]
})

print("\n Model Performance Comparison:")
print(results)

# Display detailed classification reports
print("\n Detailed Classification Report for Logistic Regression:")
print(classification_report(y_test, y_pred_log))

print("\n Detailed Classification Report for Random Forest:")
print(classification_report(y_test, y_pred_rf))

# Visualize confusion matrices
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
sns.heatmap(confusion_matrix(y_test, y_pred_log), annot=True, fmt='d', cmap='Blues')
plt.title('Logistic Regression Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')

plt.subplot(1, 3, 2)
sns.heatmap(confusion_matrix(y_test, y_pred_dt), annot=True, fmt='d', cmap='Blues')
plt.title('Decision Tree Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')

plt.subplot(1, 3, 3)
sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt='d', cmap='Blues')
plt.title('Random Forest Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')

plt.tight_layout()
plt.show()

# Select the best model based on highest F1 Score
best_model_idx = results['F1 Score'].idxmax()
best_model_name = results.loc[best_model_idx, "Model"]
print(f"\n‚úÖ Best Model Selected: {best_model_name}")


# Feature Importance Analysis for Random Forest

if best_model_name == "Random Forest":
    # Get feature importance
    importances = rf_clf.feature_importances_
    feature_imp = pd.DataFrame({'Feature': X.columns, 'Importance': importances})
    feature_imp = feature_imp.sort_values('Importance', ascending=False)

    # Plot feature importance
    plt.figure(figsize=(10, 6))
    sns.barplot(x='Importance', y='Feature', data=feature_imp.head(10))
    plt.title('Top 10 Feature Importances (Random Forest)')
    plt.tight_layout()
    plt.show()

    print("\n Top 5 Most Important Features:")
    print(feature_imp.head(5))


# Save the Best Model

model_dict = {
    "Logistic Regression": log_reg,
    "Decision Tree": dt_clf,
    "Random Forest": rf_clf,
    "K-Means Clustering": (kmeans, scaler, high_risk_cluster)  # Save with necessary components
}

best_model = model_dict[best_model_name]
model_save_path = "/content/drive/MyDrive/PYTHON PROJECT LAPD4/best_high_risk_model.sav"
pickle.dump(best_model, open(model_save_path, "wb"))
print(f"\n‚úÖ Model saved as '{model_save_path}'")

# Also save the feature names and encoder for future predictions
model_metadata = {
    "feature_names": X.columns.tolist(),
    "encoder": encoder,
    "model_type": best_model_name
}
metadata_save_path = "/content/drive/MyDrive/PYTHON PROJECT LAPD4/high_risk_model_metadata.sav"
pickle.dump(model_metadata, open(metadata_save_path, "wb"))
print(f"‚úÖ Model metadata saved as '{metadata_save_path}'")

# Install necessary libraries (if not installed)
!pip install pandas numpy plotly

# Import required libraries
import pandas as pd
import numpy as np
import pickle
import plotly.express as px
import plotly.graph_objects as go
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# üîπ Load the trained model and metadata
model_path = "/content/drive/MyDrive/PYTHON PROJECT LAPD4/best_high_risk_model.sav"
metadata_path = "/content/drive/MyDrive/PYTHON PROJECT LAPD4/high_risk_model_metadata.sav"

try:
    loaded_model = pickle.load(open(model_path, "rb"))
    print("‚úÖ Model loaded successfully")

    try:
        metadata = pickle.load(open(metadata_path, "rb"))
        print("‚úÖ Model metadata loaded successfully")
        # Extract encoder and feature names from metadata
        feature_names = metadata.get("feature_names", [])
        model_type = metadata.get("model_type", "Unknown")
        print(f"Model type: {model_type}")
    except:
        print("‚ö†Ô∏è Metadata not found or could not be loaded. Will proceed without it.")
        metadata = None
except Exception as e:
    print(f"Error loading model: {e}")
    print("‚ö†Ô∏è Proceeding with limited functionality.")
    loaded_model = None

# üîπ Load the dataset to extract feature names and prepare data
file_path = "/content/drive/MyDrive/PYTHON PROJECT LAPD4/Cleaned_Crime_Data_Weapons_Filtered_Further_Cleaned.csv"
crime_data = pd.read_csv(file_path)
print(f"‚úÖ Crime data loaded: {crime_data.shape[0]} records")

# üîπ Prepare input features (same feature engineering as training)
crime_data['DATE OCC'] = pd.to_datetime(crime_data['DATE OCC'], errors='coerce')
crime_data = crime_data.dropna(subset=['DATE OCC'])  # Remove rows with missing dates
crime_data['Year'] = crime_data['DATE OCC'].dt.year
crime_data['Month'] = crime_data['DATE OCC'].dt.month

# Extract unique areas for user selection
unique_areas = sorted(crime_data["AREA NAME"].unique().tolist())
print(f"\n‚úÖ Found {len(unique_areas)} unique areas in the dataset")

# üîπ Aggregate crime count per area and time period
crime_counts = crime_data.groupby(["Year", "Month", "AREA NAME"]).size().reset_index(name="Crime Count")
print(f"‚úÖ Aggregated data shape: {crime_counts.shape}")

# üîπ One-Hot Encode Categorical Variables
from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
encoded_features = pd.DataFrame(encoder.fit_transform(crime_counts[['AREA NAME']]))
encoded_features.columns = encoder.get_feature_names_out(['AREA NAME'])

# Join encoded features with numeric features - KEEP Year and Month
crime_counts_encoded = crime_counts[['Year', 'Month']].join(encoded_features)

# üîπ Define input features (X) - Now we're KEEPING Year and Month
X_columns = crime_counts_encoded.columns

# Interactive Area Selection
from IPython.display import display, HTML

print("\nüîπ Available Areas:")
for i, area in enumerate(unique_areas, 1):
    print(f"{i}. {area}")


# Predict High-Risk Crime Areas (Interactive Input)

try:
    selected_area_idx = int(input("\nEnter the area number (1-21): ")) - 1
    selected_area = unique_areas[selected_area_idx]

    selected_year = int(input("Enter year (e.g., 2023): "))
    selected_month = int(input("Enter month (1-12): "))

    # Create a new input row with zeros for all one-hot encoded area features
    area_columns = [col for col in X_columns if col.startswith('AREA NAME_')]
    new_input = pd.DataFrame(np.zeros((1, len(area_columns))), columns=area_columns)

    # Add Year and Month values
    new_input['Year'] = selected_year
    new_input['Month'] = selected_month

    # Set the selected area feature to 1
    area_col = f"AREA NAME_{selected_area}"
    if area_col in new_input.columns:
        new_input[area_col] = 1
    else:
        print(f"Warning: Area '{selected_area}' not found in training data columns.")

    # Ensure features are in the same order as in training
    if metadata and "feature_names" in metadata:
        expected_features = metadata["feature_names"]
        new_input = new_input.reindex(columns=expected_features, fill_value=0)

    # üîπ Make Prediction
    if loaded_model is not None:
        # Check if the model is K-Means (tuple with model, scaler, and mapping)
        if isinstance(loaded_model, tuple) and len(loaded_model) == 3:
            kmeans_model, scaler, high_risk_cluster = loaded_model
            new_input_scaled = scaler.transform(new_input)
            cluster = kmeans_model.predict(new_input_scaled)[0]
            predicted_risk = 1 if cluster == high_risk_cluster else 0
        else:
            predicted_risk = loaded_model.predict(new_input)[0]

        risk_label = "High-Risk" if predicted_risk == 1 else "Low-Risk"
        risk_color = "red" if predicted_risk == 1 else "green"
        print(f"\n‚úÖ Predicted Crime Risk for {selected_area} in {selected_month}/{selected_year}: {risk_label}")

        # Display with color
        display(HTML(f"<h3>Prediction Result:</h3><p style='font-size:20px; color:{risk_color};'>{selected_area} in {selected_month}/{selected_year} is predicted to be a <b>{risk_label}</b> area.</p>"))
    else:
        print("‚ö†Ô∏è Cannot make prediction as model could not be loaded.")
except Exception as e:
    print(f"Error making prediction: {e}")
    print("Using default values instead...")
    selected_area = unique_areas[0]
    selected_year = 2023
    selected_month = 1


# Visualize Crime Count by Area with Interactive Hover

# Aggregate data for visualization
area_crime_counts = crime_data.groupby("AREA NAME").size().reset_index(name="Total Crimes")
area_crime_counts = area_crime_counts.sort_values("Total Crimes", ascending=False)

print("\n Creating Interactive Visualization of Crime Counts by Area...")

try:
    # Interactive bar chart with hover information
    fig = px.bar(
        area_crime_counts,
        x="AREA NAME",
        y="Total Crimes",
        color="Total Crimes",
        color_continuous_scale="Viridis",
        title="Total Crime Count by Area (Hover for Details)"
    )

    # Customize hover template
    fig.update_traces(
        hovertemplate="<b>%{x}</b><br>Total Crimes: %{y:,}<extra></extra>"
    )

    fig.update_layout(
        xaxis_title="Area Name",
        yaxis_title="Total Number of Crimes",
        xaxis_tickangle=-45,
        height=600
    )

    fig.show()


    # Visualize Crime by Time Period with Interactive Hover

    # Get time-based trends
    time_crime = crime_counts.groupby(["Year", "Month", "AREA NAME"])["Crime Count"].sum().reset_index()

    # Calculate average crimes per month by area
    area_avg_crimes = time_crime.groupby("AREA NAME")["Crime Count"].mean().reset_index()
    area_avg_crimes = area_avg_crimes.rename(columns={"Crime Count": "Average Monthly Crimes"})
    area_avg_crimes = area_avg_crimes.sort_values("Average Monthly Crimes", ascending=False)

    # Create interactive visualization
    fig2 = px.bar(
        area_avg_crimes,
        x="AREA NAME",
        y="Average Monthly Crimes",
        color="Average Monthly Crimes",
        color_continuous_scale="Reds",
        title="Average Monthly Crimes by Area (Hover for Details)"
    )

    fig2.update_traces(
        hovertemplate="<b>%{x}</b><br>Average Monthly Crimes: %{y:.1f}<extra></extra>"
    )

    fig2.update_layout(
        xaxis_title="Area Name",
        yaxis_title="Average Monthly Crimes",
        xaxis_tickangle=-45,
        height=600
    )

    fig2.show()


    # Heatmap of Crime Risk by Area and Time

    # Predict risk for all areas in selected time period
    print("\nüîπ Predicting Crime Risk for All Areas...")

    if loaded_model is not None:
        risk_predictions = []

        for area in unique_areas:
            # Create input for prediction with all required features
            # First create zero-filled DataFrame for all area columns
            area_columns = [col for col in X_columns if col.startswith('AREA NAME_')]
            pred_input = pd.DataFrame(np.zeros((1, len(area_columns))), columns=area_columns)

            # Add Year and Month
            pred_input['Year'] = selected_year
            pred_input['Month'] = selected_month

            # Set the selected area to 1
            area_col = f"AREA NAME_{area}"
            if area_col in pred_input.columns:
                pred_input[area_col] = 1

            # Ensure features are in the same order as in training
            if metadata and "feature_names" in metadata:
                expected_features = metadata["feature_names"]
                pred_input = pred_input.reindex(columns=expected_features, fill_value=0)

            # Make prediction
            if isinstance(loaded_model, tuple) and len(loaded_model) == 3:
                kmeans_model, scaler, high_risk_cluster = loaded_model
                pred_input_scaled = scaler.transform(pred_input)
                cluster = kmeans_model.predict(pred_input_scaled)[0]
                risk_pred = 1 if cluster == high_risk_cluster else 0
            else:
                risk_pred = loaded_model.predict(pred_input)[0]

            # Get average crime count for this area
            avg_crime = area_avg_crimes[area_avg_crimes["AREA NAME"] == area]["Average Monthly Crimes"].values[0]

            risk_predictions.append({
                "Area": area,
                "Risk Score": risk_pred,
                "Risk Label": "High-Risk" if risk_pred == 1 else "Low-Risk",
                "Average Monthly Crimes": avg_crime
            })

        # Create dataframe and sort by risk
        risk_df = pd.DataFrame(risk_predictions)
        risk_df = risk_df.sort_values(["Risk Score", "Average Monthly Crimes"], ascending=[False, False])

        # Create interactive visualization
        fig3 = px.bar(
            risk_df,
            x="Area",
            y="Average Monthly Crimes",
            color="Risk Label",
            color_discrete_map={"High-Risk": "red", "Low-Risk": "green"},
            title=f"Crime Risk by Area (Hover for Details)",
            text="Risk Label"
        )

        # Enhanced hover information
        fig3.update_traces(
            hovertemplate="<b>%{x}</b><br>Risk: %{text}<br>Avg Monthly Crimes: %{y:.1f}<extra></extra>"
        )

        fig3.update_layout(
            xaxis_title="Area Name",
            yaxis_title="Average Monthly Crimes",
            xaxis_tickangle=-45,
            height=600,
            legend_title="Risk Category"
        )

        fig3.show()

    print("\n‚úÖ Interactive Analysis Complete!")

except Exception as e:
    print(f"Error creating interactive visualization: {e}")
    print("Ensure you have Plotly installed with: pip install plotly")

# Install necessary libraries
!pip install openai==0.28

# Import required libraries
import pandas as pd
import numpy as np
import pickle
import os
import base64
import openai
from google.colab import drive
from google.colab import files
from IPython.display import HTML, display
import datetime

# üîê Set your OpenAI API key
openai.api_key = "XXXXXXX-APIKEYHERE-XXXXXX"  # Replace with your key or load securely

# Mount Google Drive to access your files
drive.mount('/content/drive')

# Load Trained Models
crime_count_model_path = "/content/drive/MyDrive/PYTHON PROJECT LAPD4/best_crime_model.sav"
high_risk_model_path = "/content/drive/MyDrive/PYTHON PROJECT LAPD4/best_high_risk_model.sav"

try:
    crime_count_model = pickle.load(open(crime_count_model_path, "rb"))
    print("‚úÖ Crime count model loaded successfully")
    high_risk_model = pickle.load(open(high_risk_model_path, "rb"))
    print("‚úÖ High-risk model loaded successfully")
except Exception as e:
    print(f"Error loading models: {e}")

# Load Dataset for Feature Names
file_path = "/content/drive/MyDrive/PYTHON PROJECT LAPD4/Cleaned_Crime_Data_Weapons_Filtered_Further_Cleaned.csv"
try:
    crime_data = pd.read_csv(file_path)
    print(f"‚úÖ Crime data loaded: {crime_data.shape[0]} records")
except Exception as e:
    print(f"Error loading dataset: {e}")

# Prepare input features
crime_data['DATE OCC'] = pd.to_datetime(crime_data['DATE OCC'], errors='coerce')
crime_data = crime_data.dropna(subset=['DATE OCC'])
crime_data['Year'] = crime_data['DATE OCC'].dt.year
crime_data['Month'] = crime_data['DATE OCC'].dt.month
unique_areas = sorted(crime_data["AREA NAME"].unique().tolist())
print(f"\n‚úÖ Found {len(unique_areas)} unique areas in the dataset")

# One-Hot Encoding
from sklearn.preprocessing import OneHotEncoder
crime_counts = crime_data.groupby(["Year", "Month", "AREA NAME"]).size().reset_index(name="Crime Count")
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
encoded_features = pd.DataFrame(encoder.fit_transform(crime_counts[['AREA NAME']]))
encoded_features.columns = encoder.get_feature_names_out(['AREA NAME'])
crime_counts_encoded = crime_counts[['Year', 'Month']].join(encoded_features)
X_columns = crime_counts_encoded.columns.tolist()

# Prediction function
def predict_crime(area_name, year, month):
    new_input = pd.DataFrame(np.zeros((1, len(X_columns))), columns=X_columns)
    new_input['Year'] = year
    new_input['Month'] = month
    area_col = f"AREA NAME_{area_name}"
    if area_col in new_input.columns:
        new_input[area_col] = 1
    else:
        print(f"‚ö†Ô∏è Area '{area_name}' not found.")
        return None, None
    try:
        predicted_crime_count = crime_count_model.predict(new_input)[0]
        predicted_high_risk = high_risk_model.predict(new_input)[0]
        risk_label = "High-Risk" if predicted_high_risk == 1 else "Low-Risk"
        return round(predicted_crime_count), risk_label
    except Exception as e:
        print(f"Prediction error: {e}")
        return None, None

# ChatGPT Crime Report Generator
def generate_chatgpt_crime_report(area, year, month, crime_count, risk_status):
    prompt = f"""
You are an LAPD crime analyst AI. Based on the following prediction, generate:
1. A concise LAPD-style incident report.
2. 2-3 actionable recommendations.

Prediction:
- Area: {area}
- Date: {month}/{year}
- Expected Crimes: {crime_count}
- Risk Level: {risk_status}

Avoid saying this is AI-generated.
"""
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5
        )
        return response.choices[0].message['content'].strip()
    except Exception as e:
        return f"ERROR: {e}"

# Downloadable HTML/text report
def create_downloadable_report(report_text, area, year, month, crime_count, risk_status):
    current_date = datetime.datetime.now().strftime("%Y-%m-%d")
    html_content = f"""
    <html><head><style>
    body {{ font-family: Arial; margin: 40px; }}
    .header {{ background: #003366; color: #fff; padding: 20px; text-align: center; }}
    .section {{ padding: 20px; }}
    .footer {{ font-size: 0.8em; color: #666; text-align: center; }}
    </style></head><body>
    <div class='header'><h1>Crime Forecast Report</h1></div>
    <div class='section'>
        <p><b>Date Generated:</b> {current_date}</p>
        <p><b>Area:</b> {area}</p>
        <p><b>Time Frame:</b> {month}/{year}</p>
        <p><b>Predicted Crime Count:</b> {crime_count}</p>
        <p><b>Risk Status:</b> {risk_status}</p>
        <hr>
        <h3>AI-Generated Incident Report and Recommendations</h3>
        <p>{report_text}</p>
    </div>
    <div class='footer'>¬© {year} LAPD Predictive Crime Intelligence Unit</div>
    </body></html>
    """
    txt_content = f"""
LAPD CRIME FORECAST REPORT
==========================
Generated: {current_date}
Area: {area}
Month/Year: {month}/{year}
Predicted Crimes: {crime_count}
Risk: {risk_status}

REPORT:
{report_text}
"""
    return html_content, txt_content

# Download helper
def create_download_button(content, filename, label):
    b64 = base64.b64encode(content.encode()).decode()
    return f'''<a download="{filename}" href="data:text/plain;base64,{b64}" style="background:#4CAF50;color:white;padding:10px 20px;text-decoration:none;border-radius:5px">{label}</a>'''

# Main interaction
try:
    print("\nEnter parameters for crime forecast:")
    area_idx = int(input("Select area number (1 - {}): ".format(len(unique_areas)))) - 1
    area_name = unique_areas[area_idx]
    year = int(input("Enter year (e.g. 2025): "))
    month = int(input("Enter month (1-12): "))

    crime_count, risk_status = predict_crime(area_name, year, month)

    if crime_count is not None:
        print("\n‚úÖ Prediction complete. Generating report with ChatGPT...")
        report = generate_chatgpt_crime_report(area_name, year, month, crime_count, risk_status)
        print("\nüîπ ChatGPT Report:\n")
        print(report)

        html_report, txt_report = create_downloadable_report(report, area_name, year, month, crime_count, risk_status)
        html_file = f"report_{area_name.replace(' ', '_')}_{year}_{month}.html"
        txt_file = f"report_{area_name.replace(' ', '_')}_{year}_{month}.txt"

        print("\nüì• Download your reports:")
        display(HTML(create_download_button(html_report, html_file, "Download HTML Report")))
        display(HTML(create_download_button(txt_report, txt_file, "Download TXT Report")))
    else:
        print("‚ö†Ô∏è Prediction failed.")

except Exception as e:
    print(f"Unexpected error: {e}")